{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Name: Zhuoyuan Xu\n",
    "\n",
    "NetID: zx1137\n",
    "\n",
    "Due Date: March 7, 2021"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problem 1**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(a) Define $f(x)=\\alpha||x||^2_2$, then\n",
    "$$\\begin{split}prox_f(y)&=arg\\min_xf(x)+\\frac{1}{2}||x-y||^2_2\\\\&=arg\\min_x\\alpha||x||^2_2+\\frac{1}{2}||x-y||^2_2\\end{split}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both $f(x)$ and $\\frac{1}{2}||x-y||^2_2$ are convex, and then to find the proximal operator we can take the gradient of objective function in $prox_f(y)$ with respect to x and set the expression equal to 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\begin{split}\\nabla_x\\alpha||x||^2_2+\\frac{1}{2}||x-y||^2_2&=2\\alpha x+\\frac{1}{2}(2x-2y)\\\\&=2\\alpha x+x-y\\end{split}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set to 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\begin{split}2\\alpha x+x-y&=0\\\\x&=\\frac{y}{2\\alpha+1}\\end{split}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(b) In this problem, the proximity operator is$$prox_{\\alpha||·||_1}(y)=arg\\min_x\\alpha||x||_1+\\frac{1}{2}||x-y||^2_2$$\n",
    "This expression can be divided into 2 parts: $$F(x)=\\frac{1}{2}||x-y||^2_2$$ which is convex and differentiable, and $$G(x)=\\alpha||x||_1$$ which is convex but not differentiable everywhere"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we take the subgradient of the operator, and by the optimality condition we need to have\n",
    "$$0\\in\\partial(G(x)+\\frac{1}{2}||x-y||^2_2)$$ note $\\partial$ is used to denote the subgradient in this case"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus,$$x-y\\in\\partial G(x)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The i-th entry of $\\partial G(x)$ is\n",
    "$$\\begin{equation}\n",
    "  \\partial|x_i|=\\begin{cases}\n",
    "    \\alpha, & \\text{$x_i>0$}\\\\\n",
    "    -\\alpha, & \\text{$x_i<0$}\\\\\n",
    "    [-\\alpha,\\alpha], & \\text{$x_i=0$}\n",
    "  \\end{cases}\n",
    "\\end{equation}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore the $prox_{\\alpha||·||_1}(y)$ can be re-written entrywise by\n",
    "$$\\begin{equation}\n",
    "  (prox_{\\alpha||·||_1}(y))_i=\\begin{cases}\n",
    "    y_i-\\alpha, & \\text{$y_i\\geq\\alpha$}\\\\\n",
    "    0, & \\text{$|y_i|<\\alpha$}\\\\\n",
    "    y_i+\\alpha, & \\text{$y_i\\leq-\\alpha$}\n",
    "  \\end{cases}\n",
    "\\end{equation}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "which can also be written as \n",
    "$$\\begin{equation}\n",
    "  (prox_{\\alpha||·||_1}(y))_i=\\begin{cases}\n",
    "    y_i-sign(y_i)\\alpha, & \\text{$|y_i|\\geq\\alpha$}\\\\\n",
    "    0, & \\text{otherwise}\n",
    "  \\end{cases}\n",
    "\\end{equation}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This gives $prox_{\\alpha||·||_1}(y)=S_\\alpha(y)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(c) If $X\\in R^{pxn}$ has orthonormal rows ($p\\leq n$), then $XX^T=I$ and\n",
    "$$||X(y-X^T\\beta)||_2^2=||y-X^T\\beta||_2^2$$\n",
    "Thus, $$\\begin{split}arg\\min_\\beta\\frac{1}{2}||y-X^T\\beta||^2_2+f(\\beta)&=arg\\min_\\beta\\frac{1}{2}||X(y-X^T\\beta)||^2_2+f(\\beta)\\\\&=arg\\min_\\beta\\frac{1}{2}||Xy-XX^T\\beta||^2_2+f(\\beta)\\\\&=arg\\min_\\beta\\frac{1}{2}||Xy-\\beta||^2_2+f(\\beta)\\end{split}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(d) Set $w=Xy$, then the previous representation can be written as \n",
    "$$\\begin{split}arg\\min_\\beta\\frac{1}{2}||y-X^T\\beta||^2_2+f(\\beta)&=arg\\min_\\beta\\frac{1}{2}||w-\\beta||^2_2+f(\\beta)\\\\&=prox_f(w)\\end{split}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a lasso regression, from the previous problem, it has the form of $prox_f(w)$ with $$f(\\beta)=\\alpha||\\beta||_1$$ and the corresponding estimator is\n",
    "$$\\begin{equation}\n",
    "  x_i=(prox_{\\alpha||·||_1}(w))_i=\\begin{cases}\n",
    "    w_i-sign(w_i)\\alpha, & \\text{$|w_i|\\geq\\alpha$}\\\\\n",
    "    0, & \\text{otherwise}\n",
    "  \\end{cases}\n",
    "\\end{equation}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a ridge regression, from the previous problem, it has the form of $prox_f(w)$ with $$f(\\beta)=\\alpha||\\beta||_2$$ and the corresponding estimator is\n",
    "$$x=\\frac{y}{2\\alpha+1}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note in this case we have orthonormal rows in X, thus $XX^T=I$, which shows that the OLS estimator for such a matrix X can be $$\\beta_{OLS}=Xy$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both ridge and lasso regression find the $\\beta$ which is close to the OLS estimator, while penalize the $\\beta$ that variates too much. Lasso uses l1 norm to add penalty, while ridge uses l2 norm. Lasso will give sparse result while ridge will not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problem 2**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(a) Set the minimizer of $f_x$ to be $y^*$, then \n",
    "$$\\begin{split}y^*&=arg\\min_y(f(x)+\\nabla f(x)^T(y-x)+\\frac{1}{2\\alpha}||y-x||^2_2)\\\\&=arg\\min_y(\\frac{1}{2\\alpha}||y-x+\\alpha\\nabla f(x)||^2_2)\\end{split}$$\n",
    "This is a convex function and it reaches its minimum 0 when $$0\\in\\partial(||y-x+\\alpha\\nabla f(x)||^2_2)$$\n",
    "Thus,\n",
    "$$y^*=x-\\alpha\\nabla f(x)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(b) For this composite function $$h(x)=f_1(x)+f_2(x)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then the minimizer of $h(x)$ can be written as\n",
    "$$\\begin{split}x^*&=arg\\min_x h(x)\\\\&=arg\\min_x f_1(x)+f_2(x)\\end{split}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we have $f_2(x)=0$, then $x^*$ is the gradient descent update\n",
    "$$x^*=x-\\alpha\\nabla f_1(x)=arg\\min_xf_1(z)+\\nabla f_1(z)^T(x-z)+\\frac{1}{2\\alpha}||x-z||^2_2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have any $f_2(x)$ not differentiable, so \n",
    "$$\\begin{split}x^*&=arg\\min_xf_1(z)+\\nabla f_1(z)^T(x-z)+\\frac{1}{2\\alpha}||x-z||^2_2+f_2(x)\\\\&=arg\\min_x\\frac{1}{2\\alpha}||x-(z-\\alpha\\nabla f_1(z))||^2_2+f_2(x)\\end{split}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If $f_2$ has a proximal operator that is easy to compute, then we can write the expression as\n",
    "$$x^*=prox_{\\alpha,f_2}(z-\\alpha\\nabla f_1(z))$$\n",
    "In gradient descent update, to make the symbol clearer we can write as\n",
    "$$x^{k+1}=prox_{\\alpha,f_2}(x^k-\\alpha\\nabla f_1(x^k))$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(c) If $x^*$ is the optimal solution to the optimization problem, then\n",
    "$$0\\in\\nabla f_1(x^*)+\\partial f_2(x^*)$$\n",
    "note $\\partial$ represents the subgradient here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note $x^*$ is a solution to $$\\min_x\\frac{1}{2\\alpha}||x-(x^*-\\alpha\\nabla f_1(x^*))||^2_2+f_2(x)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and $$x^*=prox_{\\alpha,f_2}(x^*-\\alpha\\nabla f(x^*))$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then for $x^*$ to be a solution of this problem, we should have $0\\in\\alpha\\nabla f_1(x^*)+\\alpha\\partial f_2(x^*)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the 2 conditions are equivalent if $\\alpha>0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problem 3**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(a) The proximal gradient problem is equivalent to solve problem\n",
    "$$arg\\min_\\beta f(\\beta)=arg\\min_\\beta f_1(\\beta)+f_2(\\beta)$$ with $$f_1(\\beta)=\\frac{1}{2}||y-X\\beta||^2_2$$ which is convex and differentiable and $$f_2(\\beta)=\\lambda|\\beta|_1$$ which is convex but not differentiable everywhere"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus the problem has gradient update formula\n",
    "$$\\begin{split}\\beta_{k+1}&=prox_{\\alpha,f_2(.)}(\\beta_k-\\alpha\\nabla f_1(\\beta_k))\\\\&=S_{\\alpha,\\lambda}(\\beta_k-\\alpha\\nabla f_1(\\beta_k))\\end{split}$$\n",
    "S, similar to the problem statement in 1(b), is the soft-thresholding operator and the gradient $$\\nabla f_1(\\beta_k)=X^T(X\\beta_k-y)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(b) To check whether this expression reaches an optimum, we can check if this is a convex function. Then if it is, it reaches the optimum at a vector x if and only if the 0 vector is a subgradient at x."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$0\\in\\nabla f_1(x)+g_{f_2}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One way to account for numerical inaccuracy is that instead of let strictly 0 vector in the subgradient, we can add tolerance to the calculation. The tolerance limits the acceptable precision of the result or gives a range around 0 for the acceptable result. In this problem, we can calculate the subgradient, and check if the absolute result of the subgradient falls within the tolerance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$X(X^T\\beta-y)+\\lambda g_{f_2}=0$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$g_{f_2}=\\frac{1}{\\lambda}X(X^T\\beta-y)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These 2 expressions may be used for this problem, with $g_{f_2}$ the sign of $\\beta$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problem 4**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1st Plot: Lasso: n=340**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this plot, the n in the title is the number of training instances. The x-axis represents the value of hyperparameter $\\lambda$. The line of symbol 'o' represents the validation error against $\\lambda$ while the line of symbol 'x' the training error. Other lines on the plot are the coefficients from lasso regression along the algorithm path. We can see initially as $\\lambda$ increases, the training error increases and validation error decreases because we add a little bit bias to trade for decreasing variance. However, as $\\lambda$ becomes too large, the model becomes very bias, and thus both errors increases. The errors show horizontal segment when the lasso term dominates and the algorithm does not implement. More explanation is in the following part about the coefficients. \n",
    "\n",
    "At the same time, initially as $\\lambda$ increases, the variation in coefficients decreases. Some coefficients are forced to be 0 as $\\lambda$ increases, with others remain relatively much larger. This is why the regularization is sparse and can help dimension reduction and feature selection. Then, when lambda is very large, which happens at the same time when both training and validation reaches the maximum, all coefficients are forced to be 0. Since all coefficients are forced to be 0 after $\\lambda$ reaches the threshold, the error also stays the same."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2nd Plot**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This plot isolates the training error and validation error out from the previous plot to have a clearer graph. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3rd Plot**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This plot is the coefficients on the ridge regression path against different $\\lambda$. Different from lasso, even though ridge regression also shrinks coefficients as $\\lambda$ increases, the coefficients are not forced to be 0. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4th Plot**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This graph plots the error against different numbers of features chosen by FSR algorithm. The purple line represents training error and the red represents validation error. Initially both training and validation error decreases as number of features increases. This is reasonable because more features are added for modeling while overfitting has not become a problem yet due to lack of generality. However, as too many features are added into the mode, even though the training error decreases, the validation error increases because of overfitting. Correlated features or features that do not contain much useful information can also cause problems. The decreasing rate of training error and the increasing rate of validation error both decrease because the features that added to the model and the end are the least informative ones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5th Plot**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This plot is the performance of the lasso and ridge estimator on the given temperature data. Both test errors on the data and the 2016 data and the training errors are plotted. For all training error trends, the errors first increase as the number of training data increases, and then keeps on the level. The test errors first decrease and then keep on the same level. Initially when data size is small, more information are added to the training process which give better fit to the test data and more variation to the training. Generally, lasso prediction has better prediction for all data sizes than ridge until the point where the estimators approach the OLS solution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**6th Plot**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This plot is the regularization parameter against number of training data. As the number of training data increases, the size of the regularization parameter decreases. When we have smaller data size, the regularization effect is larger because of larger risk of overfitting, and thus larger regularization parameter is more useful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**7th Plot**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This plot is the number of features gainst number of training data. As the number of training data increases, the number of features also increases. Since we have more training data, we can fit more features which contain more information or noises. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**8th and 9th Plot**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 8th and the 9th plot are the coefficients of the ridge regression and those of the lasso estimators from the given data for different sizes of training set. The 8th plot is the ridge regression and the 9th is the lasso. All coefficients are depicted in light blue except the 3 that have the largest magnitudes. The 9th plot has a much sparser linear model except for large training set. When the size of the training set is very large, both estimators get close to the least squares solution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**10th and 11th Plot**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 10th and the 11th plot are the coefficients against regularization parameter for ridge regression and lasso estimators. The 10th plot is for ridge regression while the 11th plot is for the lasso. All coefficients are depicted in light blue except the 3 that have the largest magnitudes. In the 10th plot, ridge regression shrinks the coefficients as the regularization parameter increases, but the coefficients are not forced to 0, and the decreasing trend is relatively smooth. In the 11th plot, lasso estimator also shrinks coefficients as the regularization parameter increases, but the coefficients are all finally forced to be 0. The result of the lasso is also sparse, with the features with largest coefficients in magnitude are kept relatively large and others are shrunk to 0."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
