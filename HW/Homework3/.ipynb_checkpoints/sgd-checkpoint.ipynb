{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download data\n",
    "\n",
    "We will download data from NYU box and extract it to local disk. It may take a couple of minutes"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "! wget -O weather.zip \"https://nyu.box.com/shared/static/7m3377viu18qg8ymos0n2sjsqxl1ikex.zip\""
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "! unzip weather.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "str_path = \"./weather/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from os import listdir\n",
    "from sklearn import linear_model\n",
    "\n",
    "np.random.seed(2021)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_temp(file_name,col_ind):\n",
    "    data_aux = np.loadtxt(file_name, usecols=range(10))\n",
    "    data = data_aux[:,col_ind]\n",
    "    err_count = 0\n",
    "    ind_errs = []\n",
    "    for ind in range(data.shape[0]):\n",
    "        if data[ind] > 100 or data[ind] < -100:\n",
    "            err_count = err_count + 1\n",
    "            ind_errs.append(ind)\n",
    "            data[ind] = data[ind-1]  \n",
    "    print(\"File name: \" + file_name)\n",
    "    print(\"Errors: \" + str(err_count) + \" Indices: \" + str(ind_errs))\n",
    "    return data\n",
    "\n",
    "def create_data_matrix(str_path):\n",
    "    file_name_list = listdir(str_path)\n",
    "    file_name_list.sort()\n",
    "    col_ind = 8 # 8 = last 5 minutes, 9 = average over the whole hour\n",
    "    data_matrix = []\n",
    "    ind = 0\n",
    "    for file_name in file_name_list:\n",
    "        if file_name[0] == '.':\n",
    "            continue\n",
    "        else:\n",
    "            print(\"Station \" + str(ind))\n",
    "            ind = ind + 1\n",
    "            data_aux = extract_temp(str_path + file_name,col_ind)\n",
    "            if len(data_matrix) == 0:\n",
    "                data_matrix = data_aux\n",
    "            else:\n",
    "                data_matrix = np.vstack((data_matrix,data_aux))\n",
    "    return data_matrix.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_files = False\n",
    "if load_files:\n",
    "    str_path_2015 = str_path + \"hourly/2015/\"\n",
    "    data_matrix = create_data_matrix(str_path_2015)\n",
    "else:\n",
    "    data_matrix = np.load(str_path +\"hourly_temperature_2015.npy\")\n",
    "\n",
    "file_name_list = listdir(str_path + \"hourly/2015/\")\n",
    "file_name_list.sort()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### prepare data for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response is CRNH0203-2015-AL_Valley_Head_1_SSW.txt\n"
     ]
    }
   ],
   "source": [
    "ind_response = 18\n",
    "print(\"Response is \" + str(file_name_list[ind_response]))\n",
    "y_raw = data_matrix[:,ind_response]\n",
    "ind_X = np.hstack((np.arange(0,ind_response),np.arange(ind_response+1,data_matrix.shape[1])))\n",
    "X_raw = data_matrix[:,ind_X]\n",
    "n_features = X_raw.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_test = int(1e3)\n",
    "n_val = int(1e2)\n",
    "n_train = data_matrix.shape[0] - n_test - n_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "aux_ind = np.random.permutation(range(data_matrix.shape[0]))\n",
    "ind_test = aux_ind[:n_test]\n",
    "ind_val = aux_ind[n_test:(n_test+n_val)]\n",
    "X_test = X_raw[ind_test,:]\n",
    "y_test = y_raw[ind_test]\n",
    "X_val = X_raw[ind_val,:]\n",
    "y_val = y_raw[ind_val]\n",
    "ind_train = aux_ind[(n_test+n_val):int(n_test+n_val+n_train)]\n",
    "X_train = X_raw[ind_train,:]\n",
    "y_train = y_raw[ind_train]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this problem we will work with features that are zero mean and unit variance. We standardize the data below. Make sure to standardize the validation and test data using the statistics you compute from train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "center_vec = X_train.mean(axis=0)\n",
    "X_train_centered = X_train - center_vec\n",
    "col_norms = np.linalg.norm(X_train_centered, axis=0) / np.sqrt(n_train)\n",
    "X_train_norm = np.true_divide(X_train_centered, col_norms)\n",
    "\n",
    "y_train_center = y_train.mean()\n",
    "y_train_centered = y_train - y_train_center\n",
    "norm_y_train = np.linalg.norm(y_train_centered) / np.sqrt(n_train)\n",
    "y_train_norm = y_train_centered / norm_y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.2136898851357356"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print the error value achieved on validation set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4(d) SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stochastic_grad_descent(X, y, alpha=0.05, num_epoch=1000):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        X - the feature vector, 2D numpy array of size (num_instances, num_features)\n",
    "        y - the label vector, 1D numpy array of size (num_instances)\n",
    "        alpha - string or float, step size in gradient descent\n",
    "                NOTE: In SGD, it's not a good idea to use a fixed step size. Usually it's set to 1/sqrt(t) or 1/t\n",
    "                if alpha is a float, then the step size in every step is the float.\n",
    "                if alpha == \"1/sqrt(t)\", alpha = 1/sqrt(t).\n",
    "                if alpha == \"1/t\", alpha = 1/t.\n",
    "        num_epoch - number of epochs to go through the whole training set\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rates = [0.0005, 0.005, 0.05, \"1/sqrt(t)\", \"1/t\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# graphs of validation and training error with epoch for different learning rates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4(e) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare the test loss for SGD and 4(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
